---
title: "Project Report for Machine Learning Class"
date: "26 October 2014"
output: html_document
---

## Executive Summary
The provided data set contains measures taken from accelerometers placed in belt, arm, dumbbell and forearm of athletes performing  barbell lifts correctly and incorrectly in different ways. The "classe" variable defines the way of execution of the exercise and is used as our predicted variable.

We provided two random forest models to predict the "classe" variable. At first, we cleaned the model from variables that could not be used for our model. Then, in order to further reduce the complexity of the data set, we applied Principal Component Analysis (PCA) to the data.

Finally, we produced two random forest models, one using the PCA decomposition and one without considering the decomposition. We computed the in-sample and out-sample accuracy for both models, considering a 70% - 30% split for training and testing sets. Then we used both models to predict the output for the "20 observation data set" provided by the assignment.

The non-PCA model proved to be the more accurate between the two models. On the grading data set, the PCA RF-model predicted correctly 19/20 of the observation while the non-PCA RF-model predicted correctly 20/20 observations.


## Data Preprocessing
The first step of our analysis consisted in the loading of the data set in R variables. We separarated the data set into two pieces:

* the complete dataset provided by the project (projectSet), which contains nearly 20000 observations
* the dataset on which the assignment is graded upon (gradingSet), which contains 20 observations

```{r preProcessing, cache=TRUE}
library(caret);
library(randomForest);
projectSet <- read.csv("./data/pml-training.csv")
gradingSet <- read.csv("./data/pml-testing.csv")
```

In order to decide the transformations needed on the data set and to select the feature to incude in the model, we documented on the website http://groupware.les.inf.puc-rio.br/har, and we printed a summary of the dataset.

```{r dataSummary, results='hide'}
summary(projectSet)
str(projectSet)
```

From the inspection of the data we noticed that some variables are not significant to our analysis:

* some variables are non-numeric and contain calculation errors (such as #DIV/0!)
* some variables are "almost always" NA
* some columns are incremental or timestamps

Thus, we defined a set of "selectedColumns" that we are willing to keep in our dataset.
```{r columnsSelection}
# factor columns are not significant (except "classe")
colFactors <- (sapply(projectSet, class) == "factor")
# columns with more than 90% NAs are not significat
colNAs <- colSums(is.na(projectSet))/nrow(projectSet)
selectedColumns <- colnames(projectSet)[(colFactors == FALSE) & (colNAs < 0.9)]
# the first 4 columns are incremental and timestamp data: they can be removed
selectedColumns <- selectedColumns[-c(1,2,3,4)]
rm(colNAs); rm(colFactors);
selectedColumns
```

The remaining "selected" columns are significant and capture the problem features: they mostly correspond to position and acceleration variables of belt, arm, dumbbell and forearm of the testing subject.

Thus, we redefine the reduced (variables suffixed by "Red") datasets as follows:
```{r reduceSets}
# classe is reintroduced since it is our algorithm's output
projectSetRed <- projectSet[c("classe", selectedColumns)]
gradingSetRed <- gradingSet[selectedColumns]
```

## Cross-Validation
We decided to divide our project data set into a training set and a testing set. We applied a 70% - 30% splitting of the data in order to define a training and testing set.

```{r defineCrossValidSets}
library(caret);
set.seed(7777)
inTrain <- createDataPartition(y = projectSetRed$classe, p = 0.7, list = FALSE)
trainingSet <- projectSetRed[inTrain,]
testingSet <- projectSetRed[-inTrain,]
```

## Principal Component Analysis
In order to further reduce our problem size, we applied Principal Component Analysis to our data sets. We used the "thresh" parameter to constrain the total variance of the model captured by the PCA transformation of the data to 99%.

```{r findPCA}
# removing "classe" column, that is the first one in the data set
pcaPreProcessing <- preProcess(trainingSet[,-1], method = "pca", thresh=.99)
pcaPreProcessing
```

Using the preprocessing defined above, we can transform our training set, our test set, and the grading set.

```{r applyPCA}
trainingSetPreProc <- predict(pcaPreProcessing, trainingSet[-1])
testingSetPreProc <- predict(pcaPreProcessing, testingSet[-1])
gradingSetPreProc <- predict(pcaPreProcessing, gradingSetRed)
# adding back "classe" variable
trainingSetPreProc <- cbind(trainingSet$classe, trainingSetPreProc)
colnames(trainingSetPreProc)[1] <- "classe"
testingSetPreProc <- cbind(testingSet$classe, testingSetPreProc)
colnames(testingSetPreProc)[1] <- "classe"
```

## Random Forest Models
We computed two random forest models, the first one using the PCA decomposition and the second one without using it.

```{r randomForestModel, cache=TRUE}
modelRF_PCA <- randomForest(classe ~ ., data = trainingSetPreProc)
modelRF_PCA
modelRF_noPCA <- randomForest(classe ~ ., data = trainingSet)
modelRF_noPCA
```


## In-Sample Error and Out-of-Sample Accuracy
We computed accuracy values on training set and testing set.

In the PCA model, the values are computed as follows:

```{r computeErrorsPCAmodel}
library(caret); library(randomForest);
# confusion matrix for training set under PCA RF-model
table(trainingSetPreProc$classe, predict(modelRF_PCA, trainingSetPreProc))
inSamplePCA_accuracy <- sum(trainingSetPreProc$classe == predict(modelRF_PCA, trainingSetPreProc)) / nrow(testingSetPreProc)
# confusion matrix for testing set under PCA RF-model
table(testingSetPreProc$classe, predict(modelRF_PCA, testingSetPreProc))
outSamplePCA_accuracy <- sum(testingSetPreProc$classe == predict(modelRF_PCA, testingSetPreProc)) / nrow(testingSetPreProc)
```

Hence, in-sample accuracy for the PCA random forest model is `r inSamplePCA_accuracy*100` % and out-sample accuracy is `r outSamplePCA_accuracy*100` %. This difference could be due to an overfitting phenomenon.


```{r computeErrorsNonPCAmodel}
# confusion matrix for training set under non-PCA RF-model
table(trainingSet$classe, predict(modelRF_noPCA, trainingSet))
inSampleNoPCA_accuracy <- sum(trainingSet$classe == predict(modelRF_noPCA, trainingSet)) / nrow(trainingSet)
# confusion matrix for testing set under non-PCA RF-model
table(testingSet$classe, predict(modelRF_noPCA, testingSet))
outSampleNoPCA_accuracy <- sum(testingSet$classe == predict(modelRF_noPCA, testingSet)) / nrow(testingSet)
```

The in-sample accuracy for the PCA random forest model is `r inSampleNoPCA_accuracy*100` % and out-sample accuracy is `r outSampleNoPCA_accuracy*100` %.


## Prediction for the grading set
Finally, we applied the selected models to the data set containing 20 observations. Since the non-PCA model proved to be more accurate, in case of different outputs, we preferred the one provided by the non-PCA model.

``` {r final20solution}
predict(modelRF_PCA, gradingSetPreProc)
predict(modelRF_noPCA, gradingSetRed)
```

The solution provided by the second model proved to be correct (since it was correctly evaluated by the grader).
